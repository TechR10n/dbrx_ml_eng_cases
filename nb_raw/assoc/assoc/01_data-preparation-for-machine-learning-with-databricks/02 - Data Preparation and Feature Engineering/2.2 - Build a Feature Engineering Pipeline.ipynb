{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f050f32-9fb5-4012-9343-bfb4b9a15828",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3be99269-27a0-481e-be26-b3ad888f36df",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Build a Feature Engineering Pipeline\n",
    "\n",
    "In this demo, we will be constructing a feature engineering pipeline to manage data loading, imputation, and transformation. The pipeline will be applied to the training, testing, and validation sets, with the results showcased. The final step involves saving the pipeline to disk for future use, ensuring efficient and consistent data preparation for machine learning tasks.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "*By the end of this demo, you will be able to:*\n",
    "\n",
    "* Create a data preparation and feature engineering pipeline with multiple steps.\n",
    "* Create a pipeline with tasks for data imputation and transformation.\n",
    "* Apply a data preparation and pipeline set to a training/modeling set and a holdout set.\n",
    "* Display the results of the transformation.\n",
    "* Save a data preparation and feature engineering pipeline for potential future use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05712720-b70d-4b6d-ae0e-a0de969519e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need to use one of the following Databricks runtime(s): **13.3.x-cpu-ml-scala2.12**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be6eae96-18c4-495d-bd89-37e90c75fda8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Classroom Setup\n",
    "\n",
    "Before starting the demo, run the provided classroom setup script. This script will define configuration variables necessary for the demo. Execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b43cba7-6542-43d4-8117-c1577c612839",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2bdb1ff-88d6-435b-a721-8deeb507d53d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains **variables such as your username, catalog name, schema name, working directory, and dataset locations**. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99be3ea1-1cf6-4ea6-b9bd-4ef9d0cf6ae4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fbe70ed-5ff6-409a-88dc-d7a27c5f3047",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Data Preparation\n",
    "\n",
    "Before building the pipeline, we will ensure consistency in the dataset by converting Integer and Boolean columns to Double data types and addressing missing values in both numeric and string columns within the **`Telco`** dataset. These are the steps we will follow in this section.\n",
    "\n",
    "1. Load dataset\n",
    "\n",
    "1. Split dataset to train and test sets\n",
    "\n",
    "1. Converting Integer and Boolean Columns to Double\n",
    "\n",
    "1. Handling Missing Values\n",
    "\n",
    "  * Numeric Columns\n",
    "\n",
    "  * String Columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc954094-6fb3-4936-8578-5ef31c7d7a06",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fa6b16b-c6e2-420c-9a2e-b7f911ac1266",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Load dataset\n",
    "dataset_path = f\"{DA.paths.datasets}/telco/telco-customer-churn-missing.csv\"\n",
    "telco_df = spark.read.csv(dataset_path, header=\"true\", inferSchema=\"true\", multiLine=\"true\", escape='\"')\n",
    "\n",
    "# Select columns of interest\n",
    "telco_df = telco_df.select(\"gender\", \"SeniorCitizen\", \"Partner\", \"tenure\", \"InternetService\", \"Contract\", \"PaperlessBilling\", \"PaymentMethod\", \"TotalCharges\", \"Churn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfa3a66a-4fc4-4c5d-a24a-d41e079285dc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Quick pre-processing\n",
    "* `SeniorCitizen` as `boolean`\n",
    "* `TotalCharges` as `double`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2816b661-fd44-4130-b0ee-7ccb926d2172",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# replace \"null\" values with Null\n",
    "# for column in telco_df.columns:\n",
    "#   telco_df = telco_df.withColumn(column, when(col(column) == \"null\", None).otherwise(col(column)))\n",
    "\n",
    "# clean-up columns\n",
    "telco_df = telco_df.withColumn(\"SeniorCitizen\", when(col(\"SeniorCitizen\")==1, True).otherwise(False))\n",
    "telco_df = telco_df.withColumn(\"TotalCharges\", col(\"TotalCharges\").cast(\"double\"))\n",
    "\n",
    "display(telco_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5127356-b477-429d-a2a1-6629072c0511",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f1a70a8-9da3-4987-bdf7-ba51c748f32f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = telco_df.randomSplit([.8, .2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f32d3bc1-95b8-4714-8cb4-ec884ff2b464",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Transform Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f62d136-10fe-4a02-9bf3-cd8961108312",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, BooleanType, StringType, DoubleType\n",
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "\n",
    "# Get a list of integer & boolean columns\n",
    "integer_cols = [column.name for column in train_df.schema.fields if (column.dataType == IntegerType() or column.dataType == BooleanType())]\n",
    "\n",
    "# Loop through integer columns to cast each one to double\n",
    "for column in integer_cols:\n",
    "    train_df = train_df.withColumn(column, col(column).cast(\"double\"))\n",
    "    test_df = test_df.withColumn(column, col(column).cast(\"double\"))\n",
    "\n",
    "string_cols = [c.name for c in train_df.schema.fields if c.dataType == StringType()]\n",
    "num_cols = [c.name for c in train_df.schema.fields if c.dataType == DoubleType()]\n",
    "\n",
    "# Get a list of columns with missing values\n",
    "# Numerical\n",
    "num_missing_values_logic = [count(when(col(column).isNull(),column)).alias(column) for column in num_cols]\n",
    "row_dict_num = train_df.select(num_missing_values_logic).first().asDict()\n",
    "num_missing_cols = [column for column in row_dict_num if row_dict_num[column] > 0]\n",
    "\n",
    "# String\n",
    "string_missing_values_logic = [count(when(col(column).isNull(),column)).alias(column) for column in string_cols]\n",
    "row_dict_string = train_df.select(string_missing_values_logic).first().asDict()\n",
    "string_missing_cols = [column for column in row_dict_string if row_dict_string[column] > 0]\n",
    "\n",
    "print(f\"Numeric columns with missing values: {num_missing_cols}\")\n",
    "print(f\"String columns with missing values: {string_missing_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "323d4fab-2b9e-4d03-9f8f-74667a1d4295",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Create a Pipeline\n",
    "\n",
    "Defines a Spark ML pipeline for preprocessing a dataset, including indexing categorical columns, imputing missing values, scaling numerical features, performing one-hot encoding on categorical features, and assembling the final feature vector for machine learning.\n",
    "\n",
    "In this Spark ML pipeline, we preprocess a dataset for predicting customer churn in a telecommunications **`telco`** company. The pipeline includes the following key steps:\n",
    "\n",
    "* **Convert Categorical Columns to Numerical Indices:**\n",
    "This step converts categorical columns to numerical indices, allowing the model to process categorical data.\n",
    "\n",
    "* **Impute Missing Values:**\n",
    "The Imputer is used to fill in missing values in **numerical columns with missing values (e.g. `tenure`, `TotalCharges`) using the `mean` strategy**, ensuring that the dataset is complete and ready for analysis. \n",
    "**Missing categorical values will be automatically encoded as a separate category.**\n",
    "\n",
    "* **VectorAssembler and RobustScaler:**\n",
    "These steps combine relevant numerical columns into a feature vector and then scale the features to reduce sensitivity to outliers.\n",
    "\n",
    "* **Perform One Hot Encoding on Categorical variable:** \n",
    "This step converts the indexed categorical columns into binary sparse vectors, enabling the model to process categorical data effectively.\n",
    "\n",
    "* **Pipeline:**\n",
    " All these steps are encapsulated in a Pipeline, providing a convenient and reproducible way to preprocess the data for machine learning tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "358f04bd-ba4e-4692-bd38-a07276dbb085",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Imputer, VectorAssembler, RobustScaler, StringIndexer, OneHotEncoder\n",
    "\n",
    "\n",
    "# Imputer (mean strategy for all double/numeric)\n",
    "to_impute = num_missing_cols\n",
    "imputer = Imputer(inputCols=to_impute, outputCols=to_impute, strategy='mode')\n",
    "\n",
    "# Scale numerical\n",
    "numerical_assembler = VectorAssembler(inputCols=num_cols, outputCol=\"numerical_assembled\")\n",
    "numerical_scaler = RobustScaler(inputCol=\"numerical_assembled\", outputCol=\"numerical_scaled\")\n",
    "\n",
    "# String/Cat Indexer (will encode missing/null as separate index)\n",
    "string_cols_indexed = [c + '_index' for c in string_cols]\n",
    "string_indexer = StringIndexer(inputCols=string_cols, outputCols=string_cols_indexed, handleInvalid=\"keep\")\n",
    "\n",
    "# OHE categoricals\n",
    "ohe_cols = [column + '_ohe' for column in string_cols]\n",
    "one_hot_encoder = OneHotEncoder(inputCols=string_cols_indexed, outputCols=ohe_cols, handleInvalid=\"keep\")\n",
    "\n",
    "# Assembler (All)\n",
    "feature_cols = [\"numerical_scaled\"] + ohe_cols\n",
    "vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Instantiate the pipeline\n",
    "stages_list = [\n",
    "    imputer,\n",
    "    numerical_assembler,\n",
    "    numerical_scaler,\n",
    "    string_indexer,\n",
    "    one_hot_encoder,\n",
    "    vector_assembler\n",
    "]\n",
    "\n",
    "pipeline = Pipeline(stages=stages_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94ec55f2-ab99-43a8-a8f5-931c030ee839",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Fit the Pipeline\n",
    "\n",
    "In the context of machine learning and MLflow, **`fitting`** corresponds to the process of training a machine learning model on a specified dataset. \n",
    "\n",
    "In the previous step we created a pipeline. Now, we will fit a model based on the pipeline. This pipeline will index string columns, impute specified columns, scale numerical columns, one-hot-encode specified columns, and finally create a vector from all input columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93f3d86f-a419-4349-accc-2451a46a2770",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline_model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3eb6bfb9-8b08-4066-89ae-0a322ce706b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Next, we can use this model to transform, or apply, to any dataset we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43475ca8-1d1f-4eb0-82c8-f378c1eec914",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transform both training_df and test_df\n",
    "train_transformed_df = pipeline_model.transform(train_df)\n",
    "test_transformed_df = pipeline_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67971103-d134-450f-bc20-9789f9f23ce0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_transformed_df.select(\"features\").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad8a78e3-4707-47bb-a672-d94276eced18",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Save and Reuse the Pipeline\n",
    "\n",
    "Preserving the Telco Customer Churn Prediction pipeline, encompassing the model, parameters, and metadata, is vital for maintaining reproducibility, enabling version control, and facilitating collaboration among team members. This ensures a detailed record of the machine learning workflow. In this section, we will follow these steps;\n",
    "\n",
    "1. **Save the Pipeline:** Save the pipeline model, including all relevant components, to the designated artifact storage. The saved pipeline is organized within the **`spark_pipelines`** folder for clarity.\n",
    "\n",
    "1. **Explore Loaded Pipeline Stages:** Upon loading the pipeline, inspect the stages to reveal key transformations and understand the sequence of operations applied during the pipeline's execution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35738869-f220-4cb2-9514-22f89f136bed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Save the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1211f7c-cd42-475d-b8f4-87ee52d9f7cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline_model.save(f\"{DA.paths.working_dir}/spark_pipelines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b94d2d17-9a57-4960-9ee9-d9415b4f05fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Load and Use Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "591d96c5-bb3c-4842-bf24-78e0fcb6cfb8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "\n",
    "# Load the pipeline\n",
    "loaded_pipeline = PipelineModel.load(f\"{DA.paths.working_dir}/spark_pipelines\")\n",
    "\n",
    "# Show pipeline stages\n",
    "loaded_pipeline.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "681eec53-08d2-4ed5-8cc3-a728fdf5825d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's use loaded pipeline to transform the test dataset\n",
    "test_transformed_df = loaded_pipeline.transform(test_df)\n",
    "display(test_transformed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24dcb252-4b3a-40ec-b4c4-68cd9bbee239",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Clean up Classroom\n",
    "\n",
    "Run the following cell to remove lessons-specific assets created during this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1c3fcd4-e161-4768-be84-0501e669d8bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5f978d1-0d87-4887-997e-7dfca5466e29",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "In summary, the featured engineering pipeline showcased in this demo offers a systematic and consistent approach to handle data loading, imputation, and transformation. By demonstrating its application on different sets and emphasizing the importance of data preparation, the pipeline proves to be a valuable tool for efficient and reproducible machine learning tasks. \n",
    "\n",
    "The final step of saving the pipeline to disk ensures future usability, enhancing the overall effectiveness of the data preparation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8749f8cf-d498-4983-83a3-26e8049f46ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "&copy; 2024 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2.2 - Build a Feature Engineering Pipeline",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
