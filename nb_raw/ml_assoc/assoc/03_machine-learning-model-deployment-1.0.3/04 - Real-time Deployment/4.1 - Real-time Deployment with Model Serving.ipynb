{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d5652f9-9fc7-4254-8884-f82816d6dfb7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "156677db-9534-4969-b746-ca403e534de1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Real-time Deployment with Model Serving\n",
    "\n",
    "In this demo, we will focus on real-time deployment of machine learning models. ML models can be deployed using various technologies. Databricks' Model Serving is an easy to use serverless infrastructure for serving the models in real-time.\n",
    "\n",
    "First, we will fit a model **without using a feature store**. Then, we will serve the model via Model Serving. Model serving **supports both the API and the UI**. To demonstrate both methods, we will, first, serve the model using the UI and then server the model using **Databricks' Python SDK**.\n",
    "\n",
    "In the second section, we will fit a model **with feature store and we will use online features during the inference.** For online features, **Databricks' Online Tables** can be used.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "*By the end of this demo, you will be able to;*\n",
    "\n",
    "* Implement a real-time deployment REST API using Model Serving.\n",
    "\n",
    "* Serve multiple model versions to a Serverless Model Serving endpoint.\n",
    "\n",
    "* Set up an A/B testing endpoint by splitting the traffic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a110c48-f0b9-455b-8c9a-06b49f547660",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need to use one of the following Databricks runtime(s): **13.3.x-cpu-ml-scala2.12**\n",
    "\n",
    "* Online Tables must be enabled for the workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c4c2b98-8ded-4379-bae3-d0b51c5fc490",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Classroom Setup\n",
    "\n",
    "Before starting the demo, run the provided classroom setup script. This script will define configuration variables necessary for the demo. Execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f4845c1-14e2-4096-9634-714f24495a47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-sdk --upgrade\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e579c08b-1ae7-4ce5-96e6-38d4caadcad0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9786a2eb-9f58-42a9-8136-e8681d77a60c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c72d7cd-4168-4a7c-baf2-595fe206789e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"User DB Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95ebb836-8cdc-407a-a939-52128c95271e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "For this demonstration, we will use a fictional dataset from a Telecom Company, which includes customer information. This dataset encompasses **customer demographics**, including internet subscription details such as subscription plans, monthly charges and payment methods.\n",
    "\n",
    "After load the dataset, we will perform simple **data cleaning and feature selection**. \n",
    "\n",
    "In the final step, we will split the dataset to **features** and **response** sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cc9ab72-9361-4e14-8131-b9c99d8f05c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Dataset path\n",
    "dataset_p_telco = f\"{DA.paths.datasets}/telco/telco-customer-churn.csv\"\n",
    "\n",
    "# Dataset specs\n",
    "primary_key = \"customerID\"\n",
    "response = \"Churn\"\n",
    "features = [\"SeniorCitizen\", \"tenure\", \"MonthlyCharges\", \"TotalCharges\"] # Keeping numerical only for simplicity and demo purposes\n",
    "\n",
    "\n",
    "# Read dataset (and drop nan)\n",
    "# Convert all fields to double for spark compatibility\n",
    "telco_df = spark.read.csv(dataset_p_telco, inferSchema=True, header=True, multiLine=True, escape='\"')\\\n",
    "            .withColumn(\"TotalCharges\", col(\"TotalCharges\").cast('double'))\\\n",
    "            .withColumn(\"SeniorCitizen\", col(\"SeniorCitizen\").cast('double'))\\\n",
    "            .withColumn(\"Tenure\", col(\"tenure\").cast('double'))\\\n",
    "            .na.drop(how='any')\n",
    "\n",
    "# Separate features and ground-truth\n",
    "features_df = telco_df.select(primary_key, *features)\n",
    "response_df = telco_df.select(primary_key, response)\n",
    "\n",
    "# Covert data to pandas dataframes\n",
    "X_train_pdf = features_df.drop(primary_key).toPandas()\n",
    "Y_train_pdf = response_df.drop(primary_key).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "814fff79-7504-45f1-ab1d-6310e8c03962",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Fit and Register Models\n",
    "\n",
    "Before we start model deployment process, we will **fit and register two models**. These models are called **\"Champion\"** and **\"Challenger\"** and they will be served later on using Databricks Model Serving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e9ed9cd-add5-4b66-b6ad-4bcab4fd5e0f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Setup Model Registry with UC\n",
    "\n",
    "Before we start model deployment, we need to fit and register a model. In this demo, **we will log models to Unity Catalog**, which means first we need to setup the **MLflow Model Registry URI**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3afe32e3-08b4-426c-b81c-17fb763b1d4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Point to UC model registry\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "client = mlflow.MlflowClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a18be250-bca6-4775-a2e8-c7149703c318",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Fit and Register a Model with UC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e55c3f08-26b4-4cf5-86cd-64b10f3c0f04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "from mlflow.types.utils import _infer_schema\n",
    "from mlflow.models import infer_signature\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "\n",
    "model_name = f\"{DA.catalog_name}.{DA.schema_name}.ml_model\" # Use 3-level namespace\n",
    "\n",
    "def get_latest_model_version(model_name):\n",
    "    \"\"\"Helper function to get latest model version\"\"\"\n",
    "    model_version_infos = client.search_model_versions(\"name = '%s'\" % model_name)\n",
    "    return max([model_version_info.version for model_version_info in model_version_infos])\n",
    "\n",
    "def fit_and_register_model(X, Y, model_name_=model_name, random_state_=42, model_alias=None, log_with_fs=False, training_set_spec_=None):\n",
    "    \"\"\"Helper function to train and register a decision tree model\"\"\"\n",
    "\n",
    "    clf = DecisionTreeClassifier(random_state=random_state_)\n",
    "    with mlflow.start_run(run_name=\"Demo4_1-Real-Time-Deployment\") as mlflow_run:\n",
    "\n",
    "        # Enable automatic logging of input samples, metrics, parameters, and models\n",
    "        mlflow.sklearn.autolog(\n",
    "            log_input_examples=True,\n",
    "            log_models=False,\n",
    "            log_post_training_metrics=True,\n",
    "            silent=True)\n",
    "        \n",
    "        clf.fit(X, Y)\n",
    "\n",
    "        # Log model and push to registry\n",
    "        if log_with_fs:\n",
    "            # Infer output schema\n",
    "            try:\n",
    "                output_schema = _infer_schema(Y)\n",
    "            except Exception as e:\n",
    "                warnings.warn(f\"Could not infer model output schema: {e}\")\n",
    "                output_schema = None\n",
    "            \n",
    "            # Log using feature engineering client and push to registry\n",
    "            fe = FeatureEngineeringClient()\n",
    "            fe.log_model(\n",
    "                model=clf,\n",
    "                artifact_path=\"decision_tree\",\n",
    "                flavor=mlflow.sklearn,\n",
    "                training_set=training_set_spec_,\n",
    "                output_schema=output_schema,\n",
    "                registered_model_name=model_name_\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            signature = infer_signature(X, Y)\n",
    "            mlflow.sklearn.log_model(\n",
    "                clf,\n",
    "                artifact_path=\"decision_tree\",\n",
    "                signature=signature,\n",
    "                registered_model_name=model_name_\n",
    "            )\n",
    "\n",
    "        # Set model alias\n",
    "        if model_alias:\n",
    "            time.sleep(20) # Wait 20secs for model version to be created\n",
    "            client.set_registered_model_alias(model_name_, model_alias, get_latest_model_version(model_name_))\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7e2706a-f438-483c-ab20-c41b6d02e978",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_champion   = fit_and_register_model(X_train_pdf, Y_train_pdf, model_name, 42, \"Champion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3eba60d9-02b4-4b27-be77-26892a40fc88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_challenger = fit_and_register_model(X_train_pdf, Y_train_pdf, model_name, 10, \"Challenger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "973f6017-fe8e-41b5-8b06-c4553c9b1041",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Real-time A/B Testing Deployment with Model Serving\n",
    "\n",
    "Let's serve the two models we logged in the previous step using Model Serving. Model Serving supports endpoint management via the UI and the API. \n",
    "\n",
    "Below you will find instructions for using the UI and it is simpler method compared to the API. **In this demo, we will use the API to configure and create the endpoint**.\n",
    "\n",
    "**Both the UI and the API support querying created endpoints in real-time**. We will use the API to query the endpoint using a test-set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b4ba74c-13ba-4d13-a2bb-f69bdcf201ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Option 1: Serve model(s) using UI\n",
    "\n",
    "After registering the (new version(s) of the) model to the model registry. To provision a serving endpoint via UI, follow the steps below.\n",
    "\n",
    "1. In the left sidebar, click **Serving**.\n",
    "\n",
    "2. To create a new serving endpoint, click **Create serving endpoint**.   \n",
    "  \n",
    "    a. In the **Name** field, type a name for the endpoint.  \n",
    "  \n",
    "    b. Click in the **Entity** field. A dialog appears. Select **Unity catalog model**, and then select the catalog, schema, and model from the drop-down menus.  \n",
    "  \n",
    "    c. In the **Version** drop-down menu, select the version of the model to use.  \n",
    "  \n",
    "    d. Click **Confirm**.  \n",
    "  \n",
    "    e. In the **Compute Scale-out** drop-down, select Small, Medium, or Large. If you want to use GPU serving, select a GPU type from the **Compute type** drop-down menu.\n",
    "  \n",
    "    f. *[OPTIONAL]* to deploy another model (e.g. for A/B testing) click on **+Add Served Entity** and fill the above mentioned details.\n",
    "  \n",
    "    g. Click **Create**. The endpoint page opens and the endpoint creation process starts.   \n",
    "  \n",
    "See the Databricks documentation for details ([AWS](https://docs.databricks.com/machine-learning/model-serving/create-manage-serving-endpoints.html#ui-workflow)|[Azure](https://learn.microsoft.com/azure/databricks/machine-learning/model-serving/create-manage-serving-endpoints#--ui-workflow))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a3c454e-1199-417e-907e-5fa95a810760",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Option 2: Serve Model(s) Using the *Databricks Python SDK*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d1a0c39-a721-449f-bfb4-3ff2562284e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Get Models to Serve\n",
    "\n",
    "We will serve two models, therefore, we will get model version of the two models (**Champion** and **Challenger**) that we registered in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1bc6088-43a2-418d-aaac-fd9ce1a617b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_version_champion = client.get_model_version_by_alias(name=model_name, alias=\"Champion\").version # Get champion version\n",
    "model_version_challenger = client.get_model_version_by_alias(name=model_name, alias=\"Challenger\").version # Get challenger version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e07a8485-fc4f-41a5-8ccb-7c8eb46b20ed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Configure and Create Serving Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "994237a3-4ba6-498c-88c5-086d1843d7a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk.service.serving import EndpointCoreConfigInput\n",
    "\n",
    "\n",
    "# Parse model name from UC namespace\n",
    "served_model_name =  model_name.split('.')[-1]\n",
    "\n",
    "endpoint_config_dict = {\n",
    "    \"served_models\": [\n",
    "        {\n",
    "            \"model_name\": model_name,\n",
    "            \"model_version\": model_version_champion,\n",
    "            \"scale_to_zero_enabled\": True,\n",
    "            \"workload_size\": \"Small\"\n",
    "        },\n",
    "        {\n",
    "            \"model_name\": model_name,\n",
    "            \"model_version\": model_version_challenger,\n",
    "            \"scale_to_zero_enabled\": True,\n",
    "            \"workload_size\": \"Small\"\n",
    "        },\n",
    "    ],\n",
    "    \"traffic_config\": {\n",
    "        \"routes\": [\n",
    "            {\"served_model_name\": f\"{served_model_name}-{model_version_champion}\", \"traffic_percentage\": 50},\n",
    "            {\"served_model_name\": f\"{served_model_name}-{model_version_challenger}\", \"traffic_percentage\": 50},\n",
    "        ]\n",
    "    },\n",
    "    \"auto_capture_config\":{\n",
    "        \"catalog_name\": DA.catalog_name,\n",
    "        \"schema_name\": DA.schema_name,\n",
    "        \"table_name_prefix\": \"db_academy\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "endpoint_config = EndpointCoreConfigInput.from_dict(endpoint_config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b27c43a9-4588-411e-8d55-0a352af3741d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import EndpointTag\n",
    "\n",
    "\n",
    "# Create/Update endpoint and deploy model+version\n",
    "w = WorkspaceClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fc590fe-fd7a-4333-8975-bcdb3e4f0ac7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "endpoint_name = f\"ML_AS_03_Demo4_{DA.unique_name('_')}\"\n",
    "\n",
    "try:\n",
    "  w.serving_endpoints.create_and_wait(\n",
    "    name=endpoint_name,\n",
    "    config=endpoint_config,\n",
    "    tags=[EndpointTag.from_dict({\"key\": \"db_academy\", \"value\": \"serve_model_example\"})]\n",
    "  )\n",
    "  \n",
    "  print(f\"Creating endpoint {endpoint_name} with models {model_name} versions {model_version_champion} & {model_version_challenger}\")\n",
    "\n",
    "except Exception as e:\n",
    "  if \"already exists\" in e.args[0]:\n",
    "    print(f\"Endpoint with name {endpoint_name} already exists\")\n",
    "\n",
    "  else:\n",
    "    raise(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1968cf50-84dd-40d5-949a-02751ac127df",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Verify Endpoint Creation\n",
    "\n",
    "Let's verify that the endpoint is created and ready to be used for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c36a692-0753-4b9b-86cd-867166ec3658",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "endpoint = w.serving_endpoints.wait_get_serving_endpoint_not_updating(endpoint_name)\n",
    "\n",
    "assert endpoint.state.config_update.value == \"NOT_UPDATING\" and endpoint.state.ready.value == \"READY\" , \"Endpoint not ready or failed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d731d92c-ab1a-4200-a45a-516d48ca81ad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Query the Endpoint\n",
    "\n",
    "Here we will use a very simple `test-sample` to use for inference. In a real-life scenario, you would typically load this set from a table or a streaming pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0bf8681-251f-4048-9b23-b880abfafc1a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Hard-code test-sample\n",
    "dataframe_records = [\n",
    "    {\"SeniorCitizen\": 0, \"tenure\":12, \"MonthlyCharges\":65, \"TotalCharges\":800},\n",
    "    {\"SeniorCitizen\": 1, \"tenure\":24, \"MonthlyCharges\":40, \"TotalCharges\":500}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd8ce8f3-4c28-40df-842c-d380fdb344f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Inference results:\")\n",
    "query_response = w.serving_endpoints.query(name=endpoint_name, dataframe_records=dataframe_records)\n",
    "print(query_response.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f88ec207-b913-4cda-878c-a55e76e87cbb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Real-time Deployment with Online Features\n",
    "\n",
    "In the previous section we deployed a model without using feature tables. In this section **we will register and deploy a model for real-time inference with feature tables.** First, we will **deploy a model with online store integration** and then we will demonstrate **inference with online store integration**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bd6a24a-a3bf-45c1-8387-6bfc269518e6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Fit and Log the Model with Feature Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3771105a-df5a-497e-8219-0b4174a643f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureLookup, FeatureEngineeringClient\n",
    "\n",
    "\n",
    "feature_table_name = f\"{DA.catalog_name}.{DA.schema_name}.features\"\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "# Create feature table\n",
    "fe.create_table(\n",
    "    name=feature_table_name,\n",
    "    df=features_df,\n",
    "    primary_keys=[primary_key],\n",
    "    description=\"Example feature table\"\n",
    ")\n",
    "\n",
    "# Create training set based on feature lookup\n",
    "fl_handle = FeatureLookup(\n",
    "    table_name=feature_table_name,\n",
    "    lookup_key=[primary_key]\n",
    ")\n",
    "\n",
    "training_set_spec = fe.create_training_set(\n",
    "    df=response_df,\n",
    "    label=response,\n",
    "    feature_lookups=[fl_handle],\n",
    "    exclude_columns=[primary_key]\n",
    ")\n",
    "\n",
    "# Load training dataframe based on defined feature-lookup specification\n",
    "training_df = training_set_spec.load_df()\n",
    "\n",
    "# Convert data to pandas dataframes\n",
    "X_train_pdf2 = training_df.drop(primary_key, response).toPandas()\n",
    "Y_train_pdf2 = training_df.select(response).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d55f5149-eab8-4c7e-b71c-44ff5f7f913a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_fe = fit_and_register_model(X_train_pdf2, Y_train_pdf2, model_name, 20, log_with_fs=True, training_set_spec_=training_set_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf68f20b-cf5b-4a4d-9674-adc5bf08843b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Set Up Databricks Online Tables\n",
    "\n",
    "In this section, we will create an online table to serve feature table for real-time inference. Databricks Online Tables can be created and managed via the UI and the SDK. While we provided instructions for both of these methods, you can pick one option for creating the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa80c6cc-9035-4290-98de-d08625e14103",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "#### OPTION 1: Create Online Table via the UI\n",
    "\n",
    "You create an online table from the Catalog Explorer. The steps are described below. For more details, see the Databricks documentation ([AWS](https://docs.databricks.com/en/machine-learning/feature-store/online-tables.html#create)|[Azure](https://learn.microsoft.com/azure/databricks/machine-learning/feature-store/online-tables#create)). For information about required permissions, see Permissions ([AWS](https://docs.databricks.com/en/machine-learning/feature-store/online-tables.html#user-permissions)|[Azure](https://learn.microsoft.com/azure/databricks/machine-learning/feature-store/online-tables#user-permissions)).\n",
    "\n",
    "\n",
    "In **Catalog Explorer**, navigate to the source table that you want to sync to an online table. \n",
    "\n",
    "From the kebab menu, select **Create online table**.\n",
    "\n",
    "* Use the selectors in the dialog to configure the online table.\n",
    "  \n",
    "  * `Name`: Name to use for the online table in Unity Catalog.\n",
    "  \n",
    "  * `Primary Key`: Column(s) in the source table to use as primary key(s) in the online table.\n",
    "  \n",
    "  * Timeseries Key: (Optional). Column in the source table to use as timeseries key. When specified, the online table includes only the row with the latest timeseries key value for each primary key.\n",
    "  \n",
    "  * `Sync mode`:  Select **`Snapshot`** for Sync mode. Please refer to the documentation for more details about available options.\n",
    "  \n",
    "  * When you are done, click Confirm. The online table page appears.\n",
    "\n",
    "The new online table is created under the catalog, schema, and name specified in the creation dialog. In Catalog Explorer, the online table is indicated by online table icon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78798263-188c-4868-b44c-b9cbe61d4fc5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### OPTION 2: Use the Databricks SDK \n",
    "\n",
    "The first option for creating an online table the UI. The other alternative is the Databricks' [python-sdk](https://databricks-sdk-py.readthedocs.io/en/latest/workspace/catalog/online_tables.html). Let's  first define the table specifications, then create the table.\n",
    "\n",
    "**🚨 Note:** The workspace must be enabled for using the SDK for creating and managing online tables. You can run following code blocks in your workspace is enabled for this feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f9c70ed-0c07-4020-ae30-d257c664d28b",
     "showTitle": true,
     "title": "Create Online Table Specifications"
    }
   },
   "source": [
    "\n",
    "**Step1: Define table configuration:**\n",
    "\n",
    "```\n",
    "from databricks.sdk.service.catalog import OnlineTableSpec\n",
    "\n",
    "online_table_spec = OnlineTableSpec().from_dict({\n",
    "    \"source_table_full_name\": feature_table_name,\n",
    "    \"primary_key_columns\": [primary_key],\n",
    "    \"perform_full_copy\": True\n",
    "})\n",
    "```\n",
    "\n",
    "**Step2: Create the table**\n",
    "\n",
    "```\n",
    "from databricks.sdk.service.catalog import OnlineTablesAPI\n",
    "\n",
    "# Create online table\n",
    "w = WorkspaceClient()\n",
    "online_table = w.online_tables.create(\n",
    "    name=f\"{DA.catalog_name}.{DA.schema_name}.online_features\",\n",
    "    spec=online_table_spec\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fabac345-c7f6-406f-9616-0b489cd3e85d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Deploy the Model with Online Features\n",
    "\n",
    "Now that we have a model registered with feature table and we created an online feature table, we can deploy the model with Model Serving and use the online table during inference.\n",
    "\n",
    "**💡 Note:** The Model Serving **endpoint configuration and creation process is the same for serving models with or without feature tables**. The registered model metadata handles feature lookup during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7579e71e-3506-44ab-8c15-d391cf02e152",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fs_model_version = get_latest_model_version(model_name)\n",
    "\n",
    "fs_endpoint_config_dict = {\n",
    "    \"served_models\": [\n",
    "        {\n",
    "            \"model_name\": model_name,\n",
    "            \"model_version\": fs_model_version,\n",
    "            \"scale_to_zero_enabled\": True,\n",
    "            \"workload_size\": \"Small\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "fs_endpoint_config = EndpointCoreConfigInput.from_dict(fs_endpoint_config_dict)\n",
    "\n",
    "fs_endpoint_name = f\"ML_AS_03_Demo4_FS_{DA.unique_name('_')}\"\n",
    "\n",
    "try:\n",
    "  w.serving_endpoints.create_and_wait(\n",
    "    name=fs_endpoint_name,\n",
    "    config=fs_endpoint_config,\n",
    "    tags=[EndpointTag.from_dict({\"key\": \"db_academy\", \"value\": \"serve_fs_model_example\"})]\n",
    "  )\n",
    "  \n",
    "  print(f\"Creating endpoint {fs_endpoint_name} with models {model_name} versions {fs_model_version}\")\n",
    "\n",
    "except Exception as e:\n",
    "  if \"already exists\" in e.args[0]:\n",
    "    print(f\"Endpoint with name {fs_endpoint_name} already exists\")\n",
    "\n",
    "  else:\n",
    "    raise(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd5d7aa7-fef0-4bca-b8aa-100bd3d8a22f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Hard-code test-sample\n",
    "dataframe_records_lookups_only = [\n",
    "    {\"customerID\": \"0002-ORFBO\"},\n",
    "    {\"customerID\": \"0003-MKNFE\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dce1e45c-41de-441f-b356-40f49a0d8fc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"FS Inference results:\")\n",
    "query_response = w.serving_endpoints.query(name=fs_endpoint_name, dataframe_records=dataframe_records_lookups_only)\n",
    "print(query_response.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "988075dc-c3f2-4f03-a3ea-8ff16db3f76b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Clean up Classroom\n",
    "\n",
    "Run the following cell to remove lessons-specific assets created during this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d54db6e5-94fb-4c7d-b25e-80d82ef26db2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a643dc0-eb1f-485f-a8d0-d9a2e8f70b97",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "In this demo, we covered how to serve ML models in real-time using Databricks Model Serving. In the first part, we demonstrated how to serve models without feature store integration. Furthermore, we showed how to deploy two models on the same endpoint to conduct an A/B testing scenario. In the second section of the demo, we deployed a model with feature store integration using Databricks Online Tables. Additionally, we demonstrated how to use the endpoint for inference with Online Tables integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbeb5fef-b1ca-4b00-98f8-c5e62eb5858b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2024 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the \n",
    "<a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4.1 - Real-time Deployment with Model Serving",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
