{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e867755-71f9-4399-860e-eee8ed0d04a7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9680f1b-2aa2-4433-ad6d-a2439c821487",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Custom Model Deployment with Model Serving\n",
    "\n",
    "Databricks Model Serving provides an easy way of deploying ML models for real-time inference. In some cases, you may need to deploy custom pipelines for your models. An example would be implementing a pre or post processing of the inference result. \n",
    "\n",
    "In this demo, we will demonstrate how you could use **MLflow's `PythonModel`** to implement a post-processing step for your model and serve it with Model Serving.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "*By the end of this demo, you will be able to;*\n",
    "\n",
    "* Deploy a model with custom logic using Model Serving.\n",
    "\n",
    "* Create and manage serving endpoints using the API.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4411766f-7cae-4f54-9cc9-93f282f88319",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need to use one of the following Databricks runtime(s): **13.3.x-cpu-ml-scala2.12**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8326f1c-5b8b-4bb4-9d27-50e7a16e9683",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Classroom Setup\n",
    "\n",
    "Before starting the demo, run the provided classroom setup script. This script will define configuration variables necessary for the demo. Execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6af6e476-5bcb-4934-8b21-67eaada3d1e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-sdk --upgrade\n",
    "\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a679b5d3-c92c-4f23-9033-288aa3617bbb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1fee85e-06b2-4303-8a47-12d57bb5a521",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d76b5662-1c58-4b34-8497-d3d88d87791a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"User DB Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be7dc850-9e28-4e6c-ad24-9d9dafd3d65d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "For this demonstration, we will use a fictional dataset from a Telecom Company, which includes customer information. This dataset encompasses **customer demographics**, including internet subscription details such as subscription plans, monthly charges and payment methods.\n",
    "\n",
    "After load the dataset, we will perform simple **data cleaning and feature selection**. \n",
    "\n",
    "In the final step, we will split the dataset to **features** and **response** sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20cddfc2-8042-4987-ae34-3a72f011a9f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# dataset path\n",
    "dataset_p_telco = f\"{DA.paths.datasets}/telco/telco-customer-churn.csv\"\n",
    "\n",
    "# Dataset specs\n",
    "primary_key = \"customerID\"\n",
    "response = \"Churn\"\n",
    "features = [\"SeniorCitizen\", \"tenure\", \"MonthlyCharges\", \"TotalCharges\"] # Keeping numerical only for simplicity and demo purposes\n",
    "\n",
    "\n",
    "# Read dataset (and drop nan)\n",
    "# Convert all fields to double for spark compatibility\n",
    "telco_df = spark.read.csv(dataset_p_telco, inferSchema=True, header=True, multiLine=True, escape='\"')\\\n",
    "            .withColumn(\"TotalCharges\", col(\"TotalCharges\").cast('double'))\\\n",
    "            .withColumn(\"SeniorCitizen\", col(\"SeniorCitizen\").cast('double'))\\\n",
    "            .withColumn(\"Tenure\", col(\"tenure\").cast('double'))\\\n",
    "            .na.drop(how='any')\n",
    "\n",
    "# Separate features and ground-truth\n",
    "features_df = telco_df.select(primary_key, *features)\n",
    "response_df = telco_df.select(primary_key, response)\n",
    "\n",
    "# Convert data to pandas dataframes\n",
    "X_train_pdf = features_df.drop(primary_key).toPandas()\n",
    "Y_train_pdf = response_df.drop(primary_key).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eebbd00c-7df5-4bf2-ad09-31291e9569df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "\n",
    "# Point to UC model registry\n",
    "client = mlflow.MlflowClient()\n",
    "mlflow.set_registry_uri(\"databricks-uc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db4738a3-6bcf-449b-9a88-608382cc0dbc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# Read dataset (and drop nan)\n",
    "telco_df = spark.read.csv(dataset_p_telco, inferSchema=True, header=True, multiLine=True, escape='\"')\\\n",
    "            .withColumn(\"TotalCharges\", col(\"TotalCharges\").cast('double'))\\\n",
    "            .na.drop(how='any')\n",
    "\n",
    "# Separate features and ground-truth\n",
    "features_df = telco_df.select(primary_key, *features)\n",
    "response_df = telco_df.select(primary_key, response)\n",
    "\n",
    "# Convert data to pandas dataframes\n",
    "X_train_pdf = features_df.drop(primary_key).toPandas()\n",
    "Y_train_pdf = response_df.drop(primary_key).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9985ef99-cc1b-4017-9c00-97bbccc667f2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Fit and Register a Custom Model\n",
    "\n",
    "Before we start model deployment process, we will **fit and register a custom model**. \n",
    "\n",
    "Deploying custom pipeline for models typically involves following steps;\n",
    "\n",
    "1. Declare **wrapper classes** for custom models\n",
    "\n",
    "2. Train base model\n",
    "\n",
    "3. Instantiate custom model using trained base model & log to registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a908f6b-70c0-4ee4-a4c1-b22a05ba4152",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Define Wrapper Class\n",
    "\n",
    "We will use MLflow's `PythonModel` class to create a custom pipeline. `predict` function of the class implements the custom logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d23a973-71b1-4c51-ac07-06147de22965",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Model wrapper class to output labels and associated probabilities\n",
    "class CustomProbaModel(mlflow.pyfunc.PythonModel):\n",
    "    # Initialize model in the constructor\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    " \n",
    "    # Prediction function\n",
    "    def predict(self, context, model_input):\n",
    "        # Predict the probabilities and class\n",
    "        prediction_probabilities = self.model.predict_proba(model_input)\n",
    "        predictions = self.model.predict(model_input)\n",
    " \n",
    "        # Organize multiple outputs\n",
    "        class_labels = [\"No\", \"Yes\"]\n",
    "        result = pd.DataFrame(prediction_probabilities, columns=[f'prob_{label}' for label in class_labels])\n",
    "        result['prediction'] = predictions\n",
    "        \n",
    "        return result\n",
    "    \n",
    "# Dummy model outputting array\n",
    "class CustomCodeModel(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def predict(self, context, data):\n",
    "        return [ j for j in range(0, data.shape[0]) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "655d3d32-0ac9-404e-9d23-8f9fedc62bdb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Train Base Model\n",
    "\n",
    "In this step, **we will fit the model as normal**.\n",
    "\n",
    "Next, and the most important step is **wrapping the model with custom class that we created**.Then, **wrapped model is logged with MLflow**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70f0d580-2231-42b1-abc4-214f036a4b04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_pdf, Y_train_pdf, test_size=0.2, random_state=42)\n",
    " \n",
    "# Initialize and train RandomForestClassifier\n",
    "rf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b525cb6a-f2c4-4ec8-b3bd-30ba1b0ec8f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Wrap and Log the Custom Model\n",
    "Wrap the model and define the input and output schemas. From there, run and log the model using `pyfunc` flavor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "408dcda9-ae37-4fd6-87d6-5a2ccd2d8f97",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.models import infer_signature\n",
    "\n",
    "# Wrap the model in the ModelWrapper\n",
    "wrapped_model = CustomProbaModel(rf)\n",
    "\n",
    "# Define the input and output schemas\n",
    "input_example = X_train[:1]\n",
    "output_example = wrapped_model.predict([],X_train[:1])\n",
    "signature = infer_signature(X_train[:1], output_example)\n",
    " \n",
    "# Start an MLflow run and log the model\n",
    "custom_model_name = f\"{DA.catalog_name}.{DA.schema_name}.custom_ml_model\"\n",
    "with mlflow.start_run(run_name=\"Custom Model Example\"):\n",
    "    mlflow.pyfunc.log_model(\"model\", \n",
    "                            python_model=wrapped_model, \n",
    "                            input_example=input_example, \n",
    "                            signature=signature,\n",
    "                            registered_model_name=custom_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffe437f0-3327-4b81-a87c-c15f7499e1ef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Test Wrapped Model\n",
    "\n",
    "Before serving the model, let's test it and review the result to make sure the post-processing is implemented successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "889f7b58-7abc-4e7b-873b-b68f74ece0d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the model from the run\n",
    "run_id = mlflow.last_active_run().info.run_id\n",
    "loaded_model = mlflow.pyfunc.load_model(f\"runs:/{run_id}/model\")\n",
    " \n",
    "# Use the loaded model to predict on the test data\n",
    "y_test_ = loaded_model.predict(X_test)\n",
    "display(y_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0dd47e4-a337-4870-a46b-b96c36d186cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test custom code model\n",
    "custom_code_model = CustomCodeModel()\n",
    "y_cc_test = custom_code_model.predict([], X_train[:1])\n",
    "print(y_cc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44f8c6a9-9527-44e9-b69b-be2d34f07503",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Serve the Custom Model\n",
    "\n",
    "Let's serve the model with Model Serving. Here, we will use the API to create the endpoint and serving the model.\n",
    "\n",
    "Please note that you could simply use the UI for this task too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "500a9525-d329-4956-bcdf-ed2222e1ab0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk.service.serving import EndpointCoreConfigInput\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import EndpointTag\n",
    "\n",
    "\n",
    "# Create/Update endpoint and deploy model+version\n",
    "w = WorkspaceClient()\n",
    "endpoint_name = f\"ML_AS_03_Demo4_Custom_{DA.unique_name('_')}\"\n",
    "model_version = \"1\"\n",
    "endpoint_config_dict = {\n",
    "    \"served_models\": [\n",
    "        {\n",
    "            \"model_name\": custom_model_name,\n",
    "            \"model_version\": model_version,\n",
    "            \"scale_to_zero_enabled\": True,\n",
    "            \"workload_size\": \"Small\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "endpoint_config = EndpointCoreConfigInput.from_dict(endpoint_config_dict)\n",
    "\n",
    "try:\n",
    "  w.serving_endpoints.create_and_wait(\n",
    "    name=endpoint_name,\n",
    "    config=endpoint_config,\n",
    "    tags=[EndpointTag.from_dict({\"key\": \"db_academy\", \"value\": \"serve_custom_model_example\"})]\n",
    "  )\n",
    "  print(f\"Creating endpoint {endpoint_name} with models {custom_model_name} versions {model_version}\")\n",
    "\n",
    "except Exception as e:\n",
    "  if \"already exists\" in e.args[0]:\n",
    "    print(f\"Endpoint with name {endpoint_name} already exists\")\n",
    "\n",
    "  else:\n",
    "    raise(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a0cd8e5-5b0a-4d66-8a33-db922bdce4f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Query the Endpoint\n",
    "\n",
    "Now that the endpoint is ready, we can query it using the test-sample as shown below. Note that the `predictions` is returned as string (Yes/No) as we implemented in wrapper class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21830752-512b-4e4f-b417-4ee1b1bc4458",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Hard-code test-sample\n",
    "dataframe_records = [\n",
    "    {\"SeniorCitizen\": 0, \"tenure\":12, \"MonthlyCharges\":65, \"TotalCharges\":800},\n",
    "    {\"SeniorCitizen\": 1, \"tenure\":24, \"MonthlyCharges\":40, \"TotalCharges\":500}\n",
    "]\n",
    "\n",
    "print(\"Inference results:\")\n",
    "query_response = w.serving_endpoints.query(name=endpoint_name, dataframe_records=dataframe_records)\n",
    "print(query_response.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9eeea76b-b59b-430a-9c84-a5ba0b5da145",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Clean up Classroom\n",
    "\n",
    "Run the following cell to remove lessons-specific assets created during this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9a8392f-223c-470a-ab90-1017b3a74d91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69a68d96-4718-4731-b441-f3f8f65a669f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this demo, we demonstrated how to build a custom model pipeline using MLflow's `PythonModel` class and serve it with Databricks Model Serving. Firstly, we defined the wrapper class with custom post-processing logic. Next, we fitted the model as usual and wrapped it with the custom model. Finally, we deployed the model with Model Serving and queried the serving endpoint using the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "552cf03e-dcb5-4e30-8467-15950ddbad59",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2024 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the \n",
    "<a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4.2 - Custom Model Deployment with Model Serving",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
