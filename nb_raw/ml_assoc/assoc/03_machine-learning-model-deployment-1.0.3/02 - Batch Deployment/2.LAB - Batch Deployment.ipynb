{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48ba82a1-cd6b-4d8b-90c5-c97519af6798",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d96e2ca-dd04-42d4-b6c9-77dc78fea2d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# LAB - Batch Deployment\n",
    "\n",
    "Welcome to the \"Batch Deployment\" lab! This lab focuses on batch deployment of machine learning models using Databricks. you will engage in tasks related to model inference, model registry, and explore performance results for feature such as Liquid Clustering using `CLUSTER BY`.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "By the end of this lab, you will be able to;\n",
    "\n",
    "+ **Task 1: Load Dataset**\n",
    "    + Load Dataset\n",
    "    + Split the dataset to features and response sets\n",
    "\n",
    "+ **Task 2: Inference with feature table**\n",
    "\n",
    "    + Create Feature Table\n",
    "    + Setup Feature Lookups\n",
    "    + Fit and Register a Model with UC using Feature Table\n",
    "    + Perform batch inference using Feature Engineering's  **`score_batch`** method.\n",
    "\n",
    "+ **Task 3: Assess Liquid Clustering:**\n",
    "\n",
    "    + Evaluate the performance results for specific optimization techniques:\n",
    "        + Liquid Clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "784b983d-786e-41ba-8b02-d82c894b094b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need to use one of the following Databricks runtime(s): **13.3.x-cpu-ml-scala2.12**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ee88068-0543-4bee-9a8b-bb780f3dcb23",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Classroom Setup\n",
    "\n",
    "Before starting the demo, run the provided classroom setup script. This script will define configuration variables necessary for the demo. Execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34f6ece2-37a0-428a-aa3c-897979f9bc18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-02Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac7e31ad-498f-4e85-9ab4-9708a4cc3e2c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4877a72e-f3cb-43c7-aad6-5f30abbbec65",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"User DB Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "063d6afc-cb06-4b98-b73a-9c937bc5ed90",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Task 1: Load Dataset\n",
    "\n",
    "+ Load a dataset:\n",
    "  + Define the dataset path\n",
    "  + Define the primary key (`customerID`), response variable (`Churn`), and feature variables (`SeniorCitizen`, `tenure`, `MonthlyCharges`, `TotalCharges`) for further processing.\n",
    "  + Read the dataset, casting relevant columns to the correct data types, and drop any rows with missing values\n",
    "+ Split the dataset into training and testing sets\n",
    "  + Separate the features and the response for the training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3deb1c27-dff8-4aa0-b724-3755331faa26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "dataset path\n",
    "dataset_p_telco = f\"{DA.paths.datasets}/telco/telco-customer-churn.csv\"\n",
    "\n",
    "# Features to use\n",
    "primary_key = \"customerID\"\n",
    "response = \"Churn\"\n",
    "features = [\"SeniorCitizen\", \"tenure\", \"MonthlyCharges\", \"TotalCharges\"]\n",
    "\n",
    "# Read dataset (and drop nan)\n",
    "telco_df = spark.read.csv(dataset_p_telco, inferSchema=True, header=True, multiLine=True, escape='\"')\\\n",
    "            .withColumn(\"TotalCharges\", col(\"TotalCharges\").cast('double'))\\\n",
    "            .withColumn(\"SeniorCitizen\", col(\"SeniorCitizen\").cast('double'))\\\n",
    "            .withColumn(\"Tenure\", col(\"tenure\").cast('double'))\\\n",
    "            .na.drop(how='any')\n",
    "\n",
    "# Split with 80 percent of the data in train_df and 20 percent of the data in test_df\n",
    "train_df, test_df = telco_df.randomSplit([.8, .2], seed=42)\n",
    "\n",
    "# Separate features and ground-truth\n",
    "features_df = train_df.select<FILL_IN>\n",
    "response_df = train_df.select<FILL_IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "805ffa71-21bf-4b6c-b2fb-7caeb0aac40f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Task 2: Inference with feature table\n",
    "In this task, you will perform batch inference using a feature table. Follow the steps below:\n",
    "\n",
    "+ **Step 1: Create Feature Table**\n",
    "\n",
    "+ **Step 2: Setup Feature Lookups**\n",
    "\n",
    "+ **Step 3: Fit and Register a Model with UC using Feature Table**\n",
    "\n",
    "+ **Step 4: Use the Model for Inference**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75a639fc-30cb-4cba-a50f-bd50640d9b2d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Step 1: Create Feature Table**\n",
    "  + Begin by creating a feature table that incorporates the relevant features for inference. This involves selecting the appropriate columns, performing any necessary transformations, and storing the resulting data in a feature table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97fe78f3-f7cf-4db0-bdf0-2224b1489735",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "\n",
    "# Prepare feature set\n",
    "features_df_all = telco_df.select(primary_key, *features)\n",
    "\n",
    "# Feature table definition\n",
    "fe = FeatureEngineeringClient()\n",
    "feature_table_name = f\"{DA.catalog_name}.{DA.schema_name}.features\"\n",
    "\n",
    "# Drop table if exists\n",
    "try:\n",
    "    fe.drop_table(name=feature_table_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create feature table\n",
    "fe.create_table(\n",
    "    <FILL_IN>\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1bedba7-1127-400e-943c-2972eca7d659",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Step 2: Setup Feature Lookups**\n",
    "  + Set up a feature lookup to create a training set from the feature table. \n",
    "  + Specify the `lookup_key` based on the columns that uniquely identify records in your feature table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78de0a29-d76f-473d-9ba6-e0264d93861d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureLookup\n",
    "\n",
    "fl_handle = <FILL_IN>\n",
    "\n",
    "# Create a training set based on feature lookup\n",
    "training_set_spec = fe.<FILL_IN>\n",
    "\n",
    "# Load training dataframe based on defined feature-lookup specification\n",
    "training_df = <FILL_IN>.load_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9ea9835-28c7-4aba-aa43-2d5892ba5b31",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Step 3: Fit and Register a Model with UC using Feature Table**\n",
    "  + Fit and register a Machine Learning Model using the created training set.\n",
    "  + Train a model on the training set and register it in the model registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d227a51-6811-4b3d-bb7b-8cf85d898f68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import warnings\n",
    "from mlflow.types.utils import _infer_schema\n",
    "\n",
    "# Point to UC model registry\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "client = mlflow.MlflowClient()\n",
    "\n",
    "# Helper function that we will use for getting latest version of a model\n",
    "def get_latest_model_version(model_name):\n",
    "    \"\"\"Helper function to get latest model version\"\"\"\n",
    "    model_version_infos = client.search_model_versions(\"name = '%s'\" % model_name)\n",
    "    return max([model_version_info.version for model_version_info in model_version_infos])\n",
    "\n",
    "# Train a sklearn Decision Tree Classification model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Covert data to pandas dataframes\n",
    "X_train_pdf = training_df.drop(<FILL_IN>, <FILL_IN>).toPandas()\n",
    "Y_train_pdf = training_df.select(<FILL_IN>).toPandas()\n",
    "clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "\n",
    "# End the active MLflow run before starting a new one\n",
    "mlflow.end_run()\n",
    "\n",
    "with mlflow.start_run(run_name=\"Model-Batch-Deployment-lab-With-FS\") as mlflow_run:\n",
    "\n",
    "    # Enable automatic logging of input samples, metrics, parameters, and models\n",
    "    mlflow.sklearn.autolog(\n",
    "        log_input_examples=True,\n",
    "        log_models=False,\n",
    "        log_post_training_metrics=True,\n",
    "        silent=True)\n",
    "    \n",
    "    clf.fit(<FILL_IN>, <FILL_IN>)\n",
    "\n",
    "    # Infer output schema\n",
    "    try:\n",
    "        output_schema = _infer_schema(<FILL_IN>)\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Could not infer model output schema: {e}\")\n",
    "        output_schema = None\n",
    "\n",
    "    model_name = f\"{DA.catalog_name}.{DA.schema_name}.ml_model\"\n",
    "    \n",
    "    # Log using feature engineering client and push to registry\n",
    "    fe.<FILL_IN>\n",
    "\n",
    "    # Set model alias (i.e. Champion)\n",
    "    client.set_registered_model_alias(<FILL_IN>, <FILL_IN>, <FILL_IN>(<FILL_IN>))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48e346c4-8dc6-418b-a5bc-86b8e4907a74",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Step 4: Use the Model for Inference**\n",
    "  + Utilize the feature engineering client's `score_batch()` method for inference.\n",
    "  + Provide the model URI and a dataframe containing primary key information for the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2d7425b-dc8c-4265-833a-20b3af5f629c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model_uri = f\"models:/<FILL_IN>\"\n",
    "# Define the test dataset\n",
    "test_features_df = test_df.select(\"FILL_IN\")\n",
    "\n",
    "# Perform batch inference using Feature Engineering's score_batch method\n",
    "result_df = fe.<FILL_IN>\n",
    "\n",
    "# Display the inference results\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "561ef059-c845-483a-b9a8-ffb8b6736239",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Task 3: Assess Liquid Clustering:\n",
    "\n",
    "Evaluate the performance results for specific optimization techniques, such as: Liquid Clustering Follow the step-wise instructions below:  \n",
    "+ **Step 1:** Create `batch_inference_liquid_clustering` table and import the following columns: `customerID`, `Churn`, `SeniorCitizen`, `tenure`, `MonthlyCharges`, `TotalCharges`, and `prediction`.\n",
    "+ **Step 2:**  Begin by assessing Liquid Clustering, an optimization technique for improving performance by physically organizing data based on a specified clustering column.\n",
    "+ **Step 3:**  Optimize the target table for Liquid Clustering.\n",
    "+ **Step 4:** Specify the `CLUSTER BY` clause with the desired columns (e.g., (customerID, tenure)) to enable Liquid Clustering on the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "637d0b9c-ba8a-4203-be92-fa89e1b8a370",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE batch_inference_liquid_clustering(\n",
    "  <FILL_IN> \n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e47ca0e3-7ab8-4dea-bcad-8f6e8b6a87cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "OPTIMIZE batch_inference_liquid_clustering;\n",
    "ALTER TABLE batch_inference_liquid_clustering\n",
    "CLUSTER BY (<FILL_IN>, <FILL_IN>);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81209d79-dfe6-4afd-b633-e46351e7feca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Clean up Classroom\n",
    "\n",
    "Run the following cell to remove lessons-specific assets created during this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2f64bd5-4329-477e-9d5e-69953765edc9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e49e40de-d330-4d2b-9b5c-028484958f67",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "This lab provides you with hands-on experience in batch deployment, covering model inference, Model Registry usage, and the impact of features like Liquid Clustering on performance. you will gain practical insights into deploying models at scale in a batch-oriented environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9902eedc-6c43-424c-9240-18d1df0cdc17",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2024 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the \n",
    "<a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2.LAB - Batch Deployment",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
