{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b106d71-0db8-410f-b345-fa37fd8464d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow==2.4.0 importlib-metadata==6.8.0 cloudpickle==2.0.0 zipp==3.16.2\n",
    "%pip install --ignore-installed Jinja2==3.1.2 markupsafe==2.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57493548-1401-4553-8868-5b1b6f8f05d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**ðŸš¨ Warning: Please don't run this notebook directly. This notebook must be used when creating the DLT pipeline. Follow the instructions listed in the \"3.1.a - Pipeline Deployment\" notebook.**\n",
    "\n",
    "**ðŸš¨ Warning:** For this notebook to successfully run, you must have;\n",
    "* Trained and logged a model to the registry (e.g. `ml_model`)\n",
    "* Set the `catalog` and `schema` to point to your own. \n",
    "* Create pipeline parameters for input data path and model name (e.g. `mlpipeline.bronze_dataset_path`& `mlpipeline.model_name`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a9a61f2-a419-4f9e-951e-a5e490ef5ca7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Inference Pipeline\n",
    "\n",
    " MLflow-trained models can be used in Delta Live Tables pipelines. MLflow models are treated as transformations in Databricks, meaning they act upon a Spark DataFrame input and return results as a Spark DataFrame. Because Delta Live Tables defines datasets against DataFrames, you can convert Apache Spark workloads that leverage MLflow to Delta Live Tables with just a few lines of code.\n",
    "\n",
    "If you already have a Python notebook calling an MLflow model, you can adapt the code to Delta Live Tables by using the `@dlt.table` decorator and ensuring functions are defined to return transformation results. For an introduction to Delta Live Tables syntax, see Tutorial: [Declare a data pipeline with Python in Delta Live Tables.](https://docs.databricks.com/en/delta-live-tables/tutorial-python.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96a8586e-7d03-440a-98d6-b8827cc64a8d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Pipeline configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd5b812e-b490-47a1-b9d5-f65e47b3f8c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_dataset_path = spark.conf.get(\"mlpipeline.bronze_dataset_path\")\n",
    "model_name = spark.conf.get(\"mlpipeline.model_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15803be5-5dda-43be-b7e1-01dc77cc83c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Inference configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eeb0f61b-5b4c-4b68-a73c-a7b994c29030",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "model_uri=f\"models:/{model_name}@DLT\" \n",
    "loaded_model_udf = mlflow.pyfunc.spark_udf(spark, model_uri=model_uri, result_type=\"string\")\n",
    "\n",
    "primary_key = \"customerID\"\n",
    "features = [\"SeniorCitizen\", \"tenure\", \"MonthlyCharges\", \"TotalCharges\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bd41155-43a6-4d67-ba27-bf04a49b5fd5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## DLT Inference code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "990663cc-dcf6-4b28-9000-1e4d712dbff8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, struct\n",
    "\n",
    "\n",
    "@dlt.table(\n",
    "  name=\"raw_inputs\",\n",
    "  comment=\"Raw inputs table\",\n",
    "  table_properties={\n",
    "    \"quality\": \"bronze\"\n",
    "  } \n",
    ")\n",
    "def raw_inputs():\n",
    "  return spark.read.csv(bronze_dataset_path, inferSchema=True, header=True, multiLine=True, escape='\"')\n",
    "  \n",
    "@dlt.table(\n",
    "  name=\"features_input\",\n",
    "  comment=\"Features table\",\n",
    "  table_properties={\n",
    "    \"quality\": \"silver\"\n",
    "  }\n",
    ")\n",
    "def features_input():\n",
    "  return (\n",
    "    dlt.read(\"raw_inputs\")\n",
    "    .select(primary_key, *features)\n",
    "    .withColumn(\"SeniorCitizen\",col(\"SeniorCitizen\").cast('double'))\n",
    "    .withColumn(\"tenure\",col(\"tenure\").cast('double'))\n",
    "    .withColumn(\"TotalCharges\",col(\"TotalCharges\").cast('double'))\n",
    "    .na.drop(how='any')\n",
    "  )\n",
    "\n",
    "@dlt.table(\n",
    "  name=\"model_predictions\",\n",
    "  comment=\"Inference table\",\n",
    "  table_properties={\n",
    "    \"quality\": \"gold\"\n",
    "  }\n",
    ")\n",
    "def model_predictions():\n",
    "  return (\n",
    "    dlt.read(\"features_input\")\n",
    "    .withColumn(\"prediction\", loaded_model_udf(struct(features)))\n",
    "  )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3.1.b - Inference Pipeline",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
