{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10a115f0-20ba-44ba-8103-0d3207f24074",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9939c81b-4a1d-48f1-9532-df1037c435e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Training Regression and Classification Models\n",
    "\n",
    "In this demo, we will guide you through essential concepts and practical applications of machine learning. The first demo will be related to fitting a regression model and the second demo will be related to classification models. In these demos, you will learn how to retreive data and fit models using notebooks. In addition, you will learn how to interpret results using visualization tools and various model metrics. \n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "*By the end of this demo, you will be able to;*\n",
    "\n",
    "* Fit a linear regression model on modeling data using the sklearn API.\n",
    "\n",
    "* Interpret the fit of an sklearn linear model’s coefficients and intercept.\n",
    "\n",
    "* Fit a decision tree model using sklearn API and training data.\n",
    "\n",
    "* Visualize an sklearn tree’s split points.\n",
    "\n",
    "* Identify which metrics are tracked by MLflow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd13f800-6556-47ca-8d9a-50ed25f89c6c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need to use one of the following Databricks runtime(s): **13.3.x-cpu-ml-scala2.12 13.3.x-scala2.12**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03eddf74-5712-4cbc-a9da-683991dfa2bb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Classroom Setup\n",
    "\n",
    "Before starting the demo, run the provided classroom setup script. This script will define configuration variables necessary for the demo. Execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72b5fa58-e1b6-4d24-85af-70937a4e805b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-01.1a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec67b6ff-bdf2-47f6-b565-2509b3d0aeba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6aa9b896-6a68-49ac-93df-e81a18b0a243",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b0519d4-2181-43fa-9a77-656fd3cd24a3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Prepare Dataset\n",
    "\n",
    "In this section, we are going to prepare the dataset for our machine learning models. The dataset we'll be working with is the **California housing dataset**. \n",
    "\n",
    "The dataset has been loaded, cleaned and saved to a **feature table**. We will read data directly from this table.\n",
    "\n",
    "Then, we will split the dataset to **train and test** sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa37af89-2a3d-4ddd-86f7-9444d2c16356",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Load Dataset\n",
    "\n",
    "This dataset contains information about housing districts in California and **aims to predict the median house value** for California districts, based on various features.\n",
    "\n",
    "While data cleaning and feature engineering is out of the scope of this demo, we will only map the `ocean_proximity` field. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70ec953a-b510-4c74-aced-7c6070baa8b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "# read data from the feature store\n",
    "table_name = f\"{DA.catalog_name}.{DA.schema_name}.ca_housing\"\n",
    "feature_data_pd = fe.read_table(name=table_name).toPandas()\n",
    "feature_data_pd = feature_data_pd.drop(columns=['unique_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c791647f-cee5-4498-b28d-d6aa6bb6cc2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ocean_proximity_mapping = {\n",
    "    'NEAR BAY': 1,\n",
    "    '<1H OCEAN': 2,\n",
    "    'INLAND': 3,\n",
    "    'NEAR OCEAN': 4,\n",
    "    'ISLAND': 5  \n",
    "}\n",
    "\n",
    "# Replace values in the DataFrame\n",
    "feature_data_pd['ocean_proximity'] = feature_data_pd['ocean_proximity'].replace(ocean_proximity_mapping).astype(float)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "feature_data_pd = feature_data_pd.fillna(0)\n",
    "\n",
    "display(feature_data_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4825bf1-2e5d-4f3e-8f2b-f7535bdc0cb2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Train / Test Split\n",
    "\n",
    "Split the dataset into training and testing sets. This is essential for evaluating the performance of machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a311cb28-0a38-46d8-b10f-53418d710211",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f\"We have {feature_data_pd.shape[0]} records in our source dataset\")\n",
    "\n",
    "# split target variable into it's own dataset\n",
    "target_col = \"median_house_value\"\n",
    "X_all = feature_data_pd.drop(labels=target_col, axis=1)\n",
    "y_all = feature_data_pd[target_col]\n",
    "\n",
    "# test / train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, train_size=0.8, random_state=42)\n",
    "print(f\"We have {X_train.shape[0]} records in our training dataset\")\n",
    "print(f\"We have {X_test.shape[0]} records in our test dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c02fe72-6f85-4012-8e0b-22bf8228b5b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Examine for Potential Co-linearity\n",
    "\n",
    "Now, let's examine the correlations between predictors to identify potential co-linearity. Understanding the relationships between different features can provide insights into the dataset and help us make informed decisions during the modeling process.\n",
    "\n",
    "Let's review the **correlation matrix** in **tabular format**. Also, we can create a **graph based on the correlation matrix** to easily inspect the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2730b802-309a-4553-9c65-ac02a3a12019",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Combine X and y into a single DataFrame for simplicity\n",
    "data = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr = data.corr()\n",
    "\n",
    "# display correlation matrix\n",
    "pd.set_option('display.max_columns', 10)\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6177e363-f7f7-4bc2-8119-2320360a96cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display correlation matrix visually\n",
    "\n",
    "# Initialize figure\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i in range(len(corr.columns)):\n",
    "    for j in range(len(corr.columns)):\n",
    "        # Determine the color based on positive or negative correlation\n",
    "        color = 'blue' if corr.iloc[i, j] > 0 else 'red'\n",
    "\n",
    "        # don't fill in circles on the diagonal\n",
    "        fill = not( i == j )\n",
    "\n",
    "        # Plot the circle with size corresponding to the absolute value of correlation\n",
    "        plt.gca().add_patch(plt.Circle((j, i), \n",
    "                                       0.5 * np.abs(corr.iloc[i, j]), \n",
    "                                       color=color, \n",
    "                                       edgecolor=color,\n",
    "                                       fill=fill,\n",
    "                                       alpha=0.5))\n",
    "\n",
    "\n",
    "\n",
    "plt.xlim(-0.5, len(corr.columns) - 0.5)\n",
    "plt.ylim(-0.5, len(corr.columns) - 0.5)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.xticks(np.arange(len(corr.columns)), corr.columns, rotation=90)\n",
    "plt.yticks(np.arange(len(corr.columns)), corr.columns)\n",
    "plt.title('Correlogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e21675e8-e808-490e-a8d0-34ccc4ec9343",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Fit a Regression Model\n",
    "\n",
    "To enhance the performance of our regression model, we'll scale our input variables so that they are on a common (standardized) scale. **Standardization ensures that each feature has a mean of 0 and a standard deviation of 1**, which can be beneficial for certain algorithms, including linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b6a3e29-7f95-4700-ab8e-ce9996b204f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "import mlflow.sklearn\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "# turn on autologging\n",
    "mlflow.sklearn.autolog(log_input_examples=True)\n",
    "\n",
    "# apply the Standard Scaler to all our input columns\n",
    "std_ct = ColumnTransformer(transformers=[(\"scaler\", StandardScaler(), [\"total_bedrooms\", \"total_rooms\", \"housing_median_age\", \"latitude\", \"longitude\", \"median_income\", \"population\", \"ocean_proximity\", \"households\"])])\n",
    "\n",
    "# pipeline to transform inputs and then pass results to the linear regression model\n",
    "lr_pl = Pipeline(steps=[\n",
    "  (\"tx_inputs\", std_ct),\n",
    "  (\"lr\", LinearRegression() )\n",
    "])\n",
    "\n",
    "# fit our model\n",
    "lr_mdl = lr_pl.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the test set\n",
    "predicted = lr_mdl.predict(X_test)\n",
    "test_r2 = r2_score(y_test, predicted)\n",
    "test_mse = mean_squared_error(y_test, predicted)\n",
    "test_rmse = sqrt(test_mse)\n",
    "test_mape = mean_absolute_percentage_error(y_test, predicted)\n",
    "print(\"Test evaluation summary:\")\n",
    "print(f\"R^2: {test_r2}\")\n",
    "print(f\"MSE: {test_mse}\")\n",
    "print(f\"RMSE: {test_rmse}\")\n",
    "print(f\"MAPE: {test_mape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "933e424d-1d86-4d9c-8d56-2aa29dbf79b4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Examine Model Result\n",
    "\n",
    "Now, let's inspect the results of our linear regression model. We'll examine both the intercept and the coefficients of the fitted model. Additionally, we'll perform a **t-test on each coefficient to assess its significance in contributing to the overall model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb328a00-0235-49f7-aaa6-1d03544e3a4f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lr_mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0db7623a-25ce-4268-a605-a7495f057613",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Extracting coefficients and intercept\n",
    "coefficients = np.append([lr_mdl.named_steps['lr'].intercept_], lr_mdl.named_steps['lr'].coef_)\n",
    "coefficient_names = ['Intercept'] + X_train.columns.to_list()\n",
    "\n",
    "# Calculating standard errors and other statistics (this is a simplified example)\n",
    "# In a real scenario, you might need to calculate these values more rigorously\n",
    "n_rows, n_cols = X_train.shape\n",
    "X_with_intercept = np.append(np.ones((n_rows, 1)), X_train, axis=1)\n",
    "var_b = test_mse * np.linalg.inv(np.dot(X_with_intercept.T, X_with_intercept)).diagonal()\n",
    "standard_errors = np.sqrt(var_b)\n",
    "t_values = coefficients / standard_errors\n",
    "p_values = [2 * (1 - stats.t.cdf(np.abs(i), (len(X_with_intercept) - 1))) for i in t_values]\n",
    "\n",
    "# Creating a DataFrame for display\n",
    "summary_df = pd.DataFrame({'Coefficient': coefficients,\n",
    "                           'Standard Error': standard_errors,\n",
    "                           't-value': t_values,\n",
    "                           'p-value': p_values},\n",
    "                          index=coefficient_names)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8b13e4e-b295-4ebc-9a02-f888dcce814a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "y_pos = np.arange(len(coefficient_names))\n",
    "plt.bar(y_pos, coefficients, align='center', alpha=0.7)\n",
    "plt.xticks(y_pos, coefficient_names, rotation=45)\n",
    "plt.ylabel('Coefficient Size')\n",
    "plt.title('Coefficients in Linear Regression')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25674fd3-6c6f-4df0-a20e-36be1ddf7ec8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Clean up Classroom\n",
    "\n",
    "Run the following cell to remove lessons-specific assets created during this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "889a1060-aa3e-44ef-b0b9-8cfcae111172",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89211a8b-c83b-4230-9942-f8ceb3541fa1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "In conclusion, this demonstration provided a comprehensive walkthrough of training a linear regression model using the scikit-learn library. We covered essential steps such as data preparation, model fitting, and result examination. Understanding the coefficients and intercept of the model is crucial for interpreting its predictive power. Moreover, we discussed the significance of each feature through t-tests, offering insights into the statistical relevance of predictors. Armed with this knowledge, practitioners can make informed decisions about the importance of variables in their regression models. This demo serves as a foundational guide for those seeking a practical understanding of linear regression modeling in a machine learning context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68e87a7f-f88d-47cd-a7ab-b9f5e2812001",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2024 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the \n",
    "<a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1.1a - Training Regression Models",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
