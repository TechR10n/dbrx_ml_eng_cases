{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf1df743-60c9-4ba7-a60b-8ef4eeb7dc18",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28b46f7e-68da-4dbb-9f64-367eeff29499",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Hyperparameter Tuning with Hyperopt\n",
    "\n",
    "In this hands-on demo, you will learn how to leverage **Hyperopt**, a powerful optimization library, for efficient model tuning. We'll guide you through the process of performing **Bayesian hyperparameter optimization, demonstrating how to define the search space, objective function, and algorithm selection**. Throughout the demo, you will utilize *MLflow* to seamlessly track the model tuning process, capturing essential information such as hyperparameters, metrics, and intermediate results. By the end of the session, you will not only grasp the principles of hyperparameter optimization but also be proficient in finding the best-tuned model using various methods such as the **MLflow API** and **MLflow UI**.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "*By the end of this demo, you will be able to;*\n",
    "\n",
    "* Utilize hyperopt for model tuning.\n",
    "\n",
    "* Perform a Bayesian hyperparameter optimization using Hyperopt.\n",
    "\n",
    "* Track model tuning process with MLflow.\n",
    "\n",
    "* Query previous runs from an experiment using the `MLFlowClient`.\n",
    "\n",
    "* Review an MLflow Experiment for the best run.\n",
    "\n",
    "* Search and retrieve the best model.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcf7df3d-ba84-49e6-ad18-aa220ad5b0d5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need to use one of the following Databricks runtime(s): **13.3.x-cpu-ml-scala2.12 13.3.x-scala2.12**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cee70d9-c3e3-4086-a8a3-05faec2a9139",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Classroom Setup\n",
    "\n",
    "Before starting the demo, run the provided classroom setup script. This script will define configuration variables necessary for the demo. Execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0d0b954-3f0c-4999-9637-6ba0c2fcb134",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb6cc1b4-81b9-4c83-be14-2c082a473315",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab4b4a08-b46e-4401-b578-4e1f811712ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8689b2ca-a2f3-4669-9801-f08b9643a708",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Prepare Dataset\n",
    "\n",
    "Before we start fitting a model, we need to prepare dataset. First, we will load dataset, then we will split it to train and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c65050e-f111-4463-a809-03650d5238df",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Load Dataset\n",
    "\n",
    "In this demo we will be using the CDC Diabetes dataset. This dataset has been loaded and loaded to a feature table. We will use this feature table to load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c61ad1f6-2388-4c48-82af-a8f0d2b1edb7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.data\n",
    "\n",
    "# load data from the feature table\n",
    "table_name = f\"{DA.catalog_name}.{DA.schema_name}.diabetes\"\n",
    "diabetes_dataset = mlflow.data.load_delta(table_name=table_name)\n",
    "diabetes_pd =diabetes_dataset.df.drop(\"unique_id\").toPandas()\n",
    "\n",
    "# review dataset and schema\n",
    "display(diabetes_pd)\n",
    "print(diabetes_pd.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee734c8f-1a25-4b66-bbb4-c9c80c21c1f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Train/Test Split\n",
    "\n",
    "Next, we will divide the dataset to training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f2ed2d3-f5a4-474b-aedc-5f18a5b8e742",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f\"We have {diabetes_pd.shape[0]} records in our source dataset\")\n",
    "\n",
    "# split target variable into it's own dataset\n",
    "target_col = \"Diabetes_binary\"\n",
    "X_all = diabetes_pd.drop(labels=target_col, axis=1)\n",
    "y_all = diabetes_pd[target_col]\n",
    "\n",
    "# test / train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, train_size=0.95, random_state=42)\n",
    "print(f\"We have {X_train.shape[0]} records in our training dataset\")\n",
    "print(f\"We have {X_test.shape[0]} records in our test dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56ce78df-7686-4657-9ab6-7645098e99c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2281e05c-e90d-46ec-98db-a25a5868d818",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Define the Hyperparameter Search Space\n",
    "\n",
    "Hyperopt uses a [Bayesian optimization algorithm](https://hyperopt.github.io/hyperopt/#algorithms) to perform a more intelligent search of the hyperparameter space. Therefore, **the initial space definition is effectively a prior distribution over the hyperparameters**, which will be used as the starting point for the Bayesian optimization process. \n",
    "\n",
    "Instead of defining a range or grid for each hyperparameter, we use [Hyperopt's parameter expressions](https://hyperopt.github.io/hyperopt/getting-started/search_spaces/#parameter-expressions) to define such prior distributions over parameter values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f550ba15-6f0e-43b6-b89e-047007da6f7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "\n",
    "dtc_param_space = {\n",
    "  'criterion': hp.choice('dtree_criterion', ['gini', 'entropy']),\n",
    "  'max_depth': hp.choice('dtree_max_depth',\n",
    "                          [None, hp.uniformint('dtree_max_depth_int', 5, 50)]),\n",
    "  'min_samples_split': hp.uniformint('dtree_min_samples_split', 2, 40),\n",
    "  'min_samples_leaf': hp.uniformint('dtree_min_samples_leaf', 1, 20)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c14ee7b-ed49-4082-bf2f-1c50110e241a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Define the Optimization Function\n",
    "\n",
    "We wrap our training code up as a function that we pass to hyperopt to optimize. The function takes a set of hyperparameter values as a `dict` and returns the validation loss score.\n",
    "\n",
    "**💡 Note:** We are using `f1` score as the cross-validated loss metric. As we goal of optimization function is to minimize the loss, we are returning `-f1`, in other words, **we want to maximize the `f1` score**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6236a491-39ae-4c62-8be9-c43cdbd65af2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "import mlflow\n",
    "import mlflow.data\n",
    "import mlflow.sklearn\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from hyperopt import STATUS_OK\n",
    "\n",
    "def tuning_objective(params):\n",
    "  # start an MLFlow run\n",
    "  with mlflow.start_run(nested=True) as mlflow_run:\n",
    "    # Enable automatic logging of input samples, metrics, parameters, and models\n",
    "    mlflow.sklearn.autolog(\n",
    "        disable=False,\n",
    "        log_input_examples=True,\n",
    "        silent=True,\n",
    "        exclusive=False)\n",
    "\n",
    "    # set up our model estimator\n",
    "    dtc = DecisionTreeClassifier(**params)\n",
    "    \n",
    "    # cross-validated on the training set\n",
    "    validation_scores = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    cv_results = cross_validate(dtc, \n",
    "                                X_train, \n",
    "                                y_train, \n",
    "                                cv=5,\n",
    "                                scoring=validation_scores)\n",
    "    # log the average cross-validated results\n",
    "    cv_score_results = {}\n",
    "    for val_score in validation_scores:\n",
    "      cv_score_results[val_score] = cv_results[f'test_{val_score}'].mean()\n",
    "      mlflow.log_metric(f\"cv_{val_score}\", cv_score_results[val_score])\n",
    "\n",
    "    # fit the model on all training data\n",
    "    dtc_mdl = dtc.fit(X_train, y_train)\n",
    "\n",
    "    # evaluate the model on the test set\n",
    "    y_pred = dtc_mdl.predict(X_test)\n",
    "    accuracy_score(y_test, y_pred)\n",
    "    precision_score(y_test, y_pred)\n",
    "    recall_score(y_test, y_pred)\n",
    "    f1_score(y_test, y_pred)\n",
    "\n",
    "    # return the negative of our cross-validated F1 score as the loss\n",
    "    return {\n",
    "      \"loss\": -cv_score_results['f1'],\n",
    "      \"status\": STATUS_OK,\n",
    "      \"run\": mlflow_run\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48e282c9-5ffe-4738-8bc6-fbf592214129",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Run in Hyperopt\n",
    "\n",
    "After defining the *objective function*, we are ready to run this function with hyperopt. \n",
    "\n",
    "As you may have noticed, tuning process will need to test many models. We are going to create an instance of **`SparkTrials()` to parallelize hyperparameter tuning trials using Spark**. This is useful for distributing the optimization process across a Spark cluster.\n",
    "\n",
    "`SparkTrials` takes a **`parallelism` parameter, which specifies how many trials are run in parallel**. This parameter will depend on the compute resources available for the cluster. You can read more about how to choose the optimal `parallelism` value in this [blog post](https://www.databricks.com/blog/2021/04/15/how-not-to-tune-your-model-with-hyperopt.html). \n",
    "\n",
    "For search algorithm, we will choose the **TPE (Tree-structured Parzen Estimator) algorithm for optimization (`algo=tpe.suggest`)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd6cd9fa-0be5-438f-b1ac-2d16b65258f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import SparkTrials, fmin, tpe\n",
    "\n",
    "# set the path for mlflow experiment\n",
    "mlflow.set_experiment(f\"/Users/{DA.username}/Demo-2.1-Hyperparameter-Tuning-with-Hyperopt\")\n",
    "\n",
    "trials = SparkTrials(parallelism=4)\n",
    "with mlflow.start_run(run_name=\"Model Tuning with Hyperopt Demo\") as parent_run:\n",
    "  fmin(tuning_objective,\n",
    "      space=dtc_param_space,\n",
    "      algo=tpe.suggest,\n",
    "      max_evals=5,  # Increase this when widening the hyperparameter search space.\n",
    "      trials=trials)\n",
    "\n",
    "best_result = trials.best_trial[\"result\"]\n",
    "best_run = best_result[\"run\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68b437fc-4c17-4581-b52c-7888f4c8c701",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Note that we used a **nested run** while tracking the tuning process. This means we can access to the *parent_run* and child runs. One of the runs we would definitely be interested in is the *best_run*. Let's check out these runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2093205-a22c-4c68-aa28-0b3c97a1eb2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parent_run.info.run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7dd83faf-000f-435f-b998-e7942d044123",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "best_run.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eae55b84-d3d6-4c10-b453-93298b0714ca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Find the Best Run\n",
    "\n",
    "In this section, we will search for registered models. There are couple ways for achieving this. We will show how to search runs using MLflow API, PySpark API and the UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99a61cf2-e656-4c2e-8f87-633c0c34e59b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Find the Best Run - MLFlow API\n",
    "\n",
    "Using the MLFlow API, you can search runs in an experiment, which returns results into a Pandas DafaFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce0fa3fd-9bbd-4347-817f-a3a27f82ed39",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.entities import ViewType\n",
    "\n",
    "# search over all runs\n",
    "hpo_runs_pd = mlflow.search_runs(\n",
    "  experiment_ids=[parent_run.info.experiment_id],\n",
    "  filter_string=f\"tags.mlflow.parentRunId = '{parent_run.info.run_id}' AND attributes.status = 'FINISHED'\",\n",
    "  run_view_type=ViewType.ACTIVE_ONLY,\n",
    "  order_by=[\"metrics.cv_f1 DESC\"]\n",
    ")\n",
    "\n",
    "display(hpo_runs_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c9d1048-8340-413e-a7c5-32acab6d0178",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Find the Best Run - PySpark API\n",
    "\n",
    "Alternatively, you can read experiment results into a PySpark DataFrame and use standard Spark expressions to search runs in an experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b3a8b2b-1109-4521-bd1c-7f225fa35932",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sfn\n",
    "\n",
    "all_experiment_runs_df = spark.read.format(\"mlflow-experiment\")\\\n",
    "  .load(parent_run.info.experiment_id)\n",
    "\n",
    "hpo_runs_df = all_experiment_runs_df.where(f\"tags['mlflow.parentRunId'] = '{parent_run.info.run_id}' AND status = 'FINISHED'\")\\\n",
    "  .withColumn(\"cv_f1\", sfn.col(\"metrics\").getItem('cv_f1'))\\\n",
    "  .orderBy(sfn.col(\"cv_f1\").desc() )\n",
    "\n",
    "display(hpo_runs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebaa9055-315c-4cd2-b778-33862cb12b11",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Find the Best Run - MLflow UI\n",
    "\n",
    "The simplest way of seeing the tuning result is to use MLflow UI. \n",
    "\n",
    "* Click on **Experiments** from left menu.\n",
    "\n",
    "* Select experiment which has the same name as this notebook's title (**2.1 - Hyperparameter Tuning with Hyperopt**).\n",
    "\n",
    "* View the **parent run** and **nested child runs**. \n",
    "\n",
    "* You can filter and order by metrics and other model metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79c2827e-71ff-49c7-90bd-72ff509f11a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Clean up Classroom\n",
    "\n",
    "Run the following cell to remove lessons-specific assets created during this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c7b2338-300d-4795-9450-81b873d0cd68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99052a66-752e-4edb-9484-c88a07c25eff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "To sum it up, this demo has shown you the process of tuning your models using Hyperopt and MLflow. You've learned a method to fine-tune your model settings through Bayesian optimization and how to keep tabs on the whole tuning journey with MLflow. Moving forward, these tools will be instrumental in improving your model's performance and simplifying the process of fine-tuning machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80ddd983-11a4-4624-ae95-af5c19473909",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2024 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the \n",
    "<a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2.1 - Hyperparameter Tuning with Hyperopt",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
